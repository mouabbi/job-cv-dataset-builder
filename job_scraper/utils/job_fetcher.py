import requests
import os
import re
import json
from utils.job_desc_scraper import scrape_full_description
from fpdf import FPDF  # Assure-toi d'avoir install√©: pip install fpdf
from dotenv import load_dotenv
load_dotenv()

# --- Identifiants Adzuna ---
APP_ID = os.getenv("ADZUNA_APP_ID")
APP_KEY = os.getenv("ADZUNA_API_KEY")
COUNTRY = "us"
RESULTS_PER_KEYWORD = 3  # üëà Tu veux seulement 3 r√©sultats par mot-cl√©

# Charger la structure des domaines depuis un fichier JSON
def load_domains(path):
    with open(path, 'r') as file:
        return json.load(file)

# Fonction pour r√©cup√©rer les offres d'emploi en fonction des mots-cl√©s
def fetch_jobs_by_keyword(keyword, max_results=RESULTS_PER_KEYWORD):
    url = (
        f"https://api.adzuna.com/v1/api/jobs/{COUNTRY}/search/1"
        f"?app_id={APP_ID}&app_key={APP_KEY}"
        f"&results_per_page={max_results}&what={keyword}"
    )
    response = requests.get(url)
    if response.status_code == 200:
        return response.json().get("results", [])
    else:
        print(f"‚ùå Erreur API {response.status_code} pour '{keyword}'")
        return []

def sanitize_filename(filename):
    # Remplacer les /, \ et autres caract√®res sp√©ciaux par _
    return re.sub(r'[\\/*?:"<>|]', '_', filename)

# Sauvegarder chaque offre d'emploi dans un fichier TXT ou PDF
def save_jobs_to_file(domain, keyword, jobs, base_path, format='txt'):
    domain_path = os.path.join(base_path, sanitize_filename(domain))
    os.makedirs(domain_path, exist_ok=True)

    for idx, job in enumerate(jobs, 1):
        title = job.get('title', 'No Title')
        company = job.get('company', {}).get('display_name', 'Unknown Company')
        location = job.get('location', {}).get('display_name', 'Unknown Location')
        link = job.get('redirect_url', '')
        
        print(f"       üîÑ scrape desc for: {link} ...")
        try:
            description = scrape_full_description(link)
        except Exception as e:
            print(f"       ‚ö†Ô∏è Erreur lors du scraping de l'offre: {e}. Lien ignor√©.")
            continue  # Skip this job and move to the next

        content = f"""Title: {title}
        Company: {company}
        Location: {location}
        
        Description:
        {description}
        """

        # Utilisation du mot-cl√© pour le nom de fichier
        sanitized_keyword = sanitize_filename(keyword)
        file_basename = f"{sanitized_keyword}_job_{idx}"

        if format == 'txt':
            file_path = os.path.join(domain_path, f"{file_basename}.txt")
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            except Exception as e:
                print(f"‚ùå Erreur lors de la sauvegarde du fichier TXT pour '{file_basename}': {e}")

        elif format == 'pdf':
            pdf = FPDF()
            pdf.add_page()
            pdf.set_auto_page_break(auto=True, margin=15)
            pdf.set_font("Arial", size=12)
            for line in content.split('\n'):
                pdf.multi_cell(0, 10, line)
            file_path = os.path.join(domain_path, f"{file_basename}.pdf")
            try:
                pdf.output(file_path)
            except Exception as e:
                print(f"‚ùå Erreur lors de la sauvegarde du fichier PDF pour '{file_basename}': {e}")

# Fonction principale
def fetch_and_save_jobs_for_domain(domain_structure, base_path, max_results=1, format='txt'):
    print("üöÄ D√©marrage de la collecte des offres...\n")
    total_domains = len(domain_structure)
    total_requests = 0  # Initialiser le compteur de requ√™tes

    for i, (domain, keywords) in enumerate(domain_structure.items(), 1):
        print(f"\nüîç [{i}/{total_domains}] Domaine: {domain} - {len(keywords)} mot(s)-cl√©(s)")
        
        for j, keyword in enumerate(keywords, 1):
            print(f"   üî∏ [{j}/{len(keywords)}] Recherche pour le mot-cl√©: '{keyword}'")
            
            jobs = fetch_jobs_by_keyword(keyword, max_results=max_results)
            nb_jobs = len(jobs)
            total_requests += 1  # Incr√©menter le compteur de requ√™tes apr√®s chaque appel API
            
            if jobs:
                print(f"     ‚úÖ {nb_jobs} offre(s) trouv√©e(s). Sauvegarde en cours...")
                save_jobs_to_file(domain, keyword, jobs, base_path, format=format)
                print(f"     üìÅ Offres enregistr√©es dans: {os.path.join(base_path, domain)}")
            else:
                print(f"     ‚ö†Ô∏è Aucune offre trouv√©e pour: {keyword}")
    
    print(f"\n‚úÖ Termin√© ! Toutes les offres ont √©t√© r√©cup√©r√©es et sauvegard√©es.")
    print(f"üî¢ Nombre total de requ√™tes ex√©cut√©es: {total_requests}")  # Afficher le nombre total de requ√™tes ex√©cut√©es
